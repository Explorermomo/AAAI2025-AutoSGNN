[
    {
         "algorithm": "A new Generalized PageRank (GPR) GNN architecture is introduced that adaptively learns GPR weights to jointly optimize node features and topological information extraction regardless of the degree of homogeneity or heterogeneity of node labels.",
         "code": "import torch\nimport numpy as np\nimport torch_geometric.utils\nfrom torch.nn import Parameter\nfrom torch_geometric.nn.conv import MessagePassing\nclass GNN_Layer(MessagePassing):\n    def __init__(self, in_channel, out_channel):\n        super(GNN_Layer, self).__init__(aggr='add')\n        self.K = 2\n        self.alpha = 0.1\n        TEMP = (self.alpha)**np.arange(self.K+1)\n        TEMP = TEMP/np.sum(np.abs(TEMP))\n        self.temp = Parameter(torch.tensor(TEMP))\n\n    def reset_parameters(self):\n        torch.nn.init.zeros_(self.temp)\n        for k in range(self.K+1):\n            self.temp.data[k] = self.alpha**k\n        self.temp.data = self.temp.data/torch.sum(torch.abs(self.temp.data))\n\n    def forward(self, x, x_raw, edge_index):\n        fill_value = 2. \n        num_nodes = x.shape[0]\n        edge_index, edge_weight = torch_geometric.utils.add_remaining_self_loops(edge_index, None, fill_value, num_nodes)\n        edge_weight = torch.ones((edge_index.size(1), ), dtype=x.dtype,device=edge_index.device)\n        row, col = edge_index[0], edge_index[1]\n        idx = col\n        deg = torch_geometric.utils.scatter(edge_weight, idx, dim=0, dim_size=num_nodes, reduce='sum')\n        deg_inv_sqrt = deg.pow_(-0.5)\n        deg_inv_sqrt.masked_fill_(deg_inv_sqrt == float('inf'), 0)\n        edge_weight = deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n        hidden = x*(self.temp[0])\n        for k in range(self.K):\n            x = self.propagate(edge_index, x=x, norm=edge_weight)\n            gamma = self.temp[k+1]\n            hidden = hidden + gamma*x\n        return hidden\n    def message(self, x_j, norm):\n        return norm.view(-1, 1) * x_j\n#code_end",
         "objective": 89.01851,
         "other_inf": null
    },
    {
         "algorithm": "A new GNN layer architecture that incorporates both attention mechanism and graph convolution operation to capture and propagate node features effectively in citation network graphs.",
         "code": "import torch\nimport torch_geometric.utils\nfrom torch.nn import Parameter\nfrom torch_geometric.nn.conv import MessagePassing\n\nclass GNN_Layer(MessagePassing):\n    def __init__(self, input_channels, output_channels):\n        super(GNN_Layer, self).__init__(aggr='add')\n        self.K = 6\n        self.alpha = 0.4\n        self.beta = Parameter(torch.tensor([0.3]*self.K))\n        self.p = Parameter(torch.tensor(0.6))\n\n    def reset_parameters(self):\n        torch.nn.init.ones_(self.beta)\n        self.p.data = 0.6\n\n    def forward(self, x, x_raw, edge_index):\n        num_nodes = x.shape[0]\n        edge_index, _ = torch_geometric.utils.add_remaining_self_loops(edge_index, None, 2., num_nodes)\n        edge_weight = torch.ones((edge_index.size(1),), dtype=x.dtype, device=edge_index.device)\n        row, col = edge_index[0], edge_index[1]\n        idx = col\n        deg = torch_geometric.utils.scatter(edge_weight, idx, dim=0, dim_size=num_nodes, reduce='sum')\n        deg_inv_sqrt = deg.pow_(-0.5)\n        deg_inv_sqrt.masked_fill_(deg_inv_sqrt == float('inf'), 0)\n        edge_weight = deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n\n        hidden = x * self.alpha + x_raw\n        for k in range(self.K):\n            x = self.propagate(edge_index, x=x, norm=edge_weight)\n            beta = torch.sigmoid(self.beta[k])\n            attention_scores = torch.sigmoid(self.p)\n            x = x * attention_scores\n            hidden = hidden + beta * x\n\n        return hidden\n\n    def message(self, x_j, norm):\n        return norm.view(-1, 1) * x_j\n\n#code_end",
         "objective": 88.80074,
         "other_inf": null
    },
    {
         "algorithm": "A novel GNN layer architecture that combines graph spectral convolutions with graph coarsening techniques and random walk-based edge representations to improve node feature learning in citation network graphs",
         "code": "import torch\nimport torch_geometric.utils\nfrom torch.nn import Parameter\nfrom torch_geometric.nn.conv import MessagePassing\n\nclass GNN_Layer(MessagePassing):\n    def __init__(self, input_channels, output_channels):\n        super(GNN_Layer, self).__init__(aggr='add')\n        self.K = 5\n        self.alpha = 0.4\n        self.beta = Parameter(torch.tensor([0.3]*self.K))\n        self.coarsening_rate = Parameter(torch.tensor(0.6))\n\n    def reset_parameters(self):\n        torch.nn.init.ones_(self.beta)\n        self.coarsening_rate.data = 0.6\n\n    def forward(self, x, x_raw, edge_index):\n        num_nodes = x.shape[0]\n        edge_index, _ = torch_geometric.utils.add_remaining_self_loops(edge_index, None, 2., num_nodes)\n        edge_weight = torch.ones((edge_index.size(1),), dtype=x.dtype, device=edge_index.device)\n        row, col = edge_index[0], edge_index[1]\n        idx = col\n        deg = torch_geometric.utils.scatter(edge_weight, idx, dim=0, dim_size=num_nodes, reduce='sum')\n        deg_inv_sqrt = deg.pow_(-0.5)\n        deg_inv_sqrt.masked_fill_(deg_inv_sqrt == float('inf'), 0)\n        edge_weight = deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n\n        hidden = x * self.alpha + x_raw\n        for k in range(self.K):\n            x = self.propagate(edge_index, x=x, norm=edge_weight)\n            beta = torch.sigmoid(self.beta[k])\n            coarsening_rate = torch.sigmoid(self.coarsening_rate)\n            x = x * coarsening_rate\n            hidden = hidden + beta * x\n\n        return hidden\n\n    def message(self, x_j, norm):\n        return norm.view(-1, 1) * x_j\n\n#code_end",
         "objective": 88.70849,
         "other_inf": null
    },
    {
         "algorithm": "\nA novel GNN layer architecture that combines graph Laplacian operators with graph pooling techniques and non-linear transformations to capture structural information and enhance node representation learning in citation network graphs.\n",
         "code": "import torch\nimport torch_geometric.utils\nfrom torch.nn import Parameter\nfrom torch_geometric.nn.conv import MessagePassing\n\nclass GNN_Layer(MessagePassing):\n    def __init__(self, input_channels, output_channels):\n        super(GNN_Layer, self).__init__(aggr='add')\n        self.K = 5\n        self.alpha = 0.4\n        self.beta = Parameter(torch.tensor([0.3]*self.K))\n        self.pooling_weight = Parameter(torch.tensor(0.6))\n\n    def reset_parameters(self):\n        torch.nn.init.ones_(self.beta)\n        self.pooling_weight.data = 0.6\n\n    def forward(self, x, x_raw, edge_index):\n        num_nodes = x.shape[0]\n        edge_index, _ = torch_geometric.utils.add_remaining_self_loops(edge_index, None, 2., num_nodes)\n        edge_weight = torch.ones((edge_index.size(1),), dtype=x.dtype, device=edge_index.device)\n        row, col = edge_index[0], edge_index[1]\n        idx = col\n        deg = torch_geometric.utils.scatter(edge_weight, idx, dim=0, dim_size=num_nodes, reduce='sum')\n        deg_inv_sqrt = deg.pow_(-0.5)\n        deg_inv_sqrt.masked_fill_(deg_inv_sqrt == float('inf'), 0)\n        edge_weight = deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n\n        hidden = x + self.alpha * x_raw\n        for k in range(self.K):\n            x = self.propagate(edge_index, x=x, norm=edge_weight)\n            beta = torch.sigmoid(self.beta[k])\n            pooling_weight = torch.sigmoid(self.pooling_weight)\n            x = x * pooling_weight\n            hidden = hidden + beta * x\n\n        return hidden\n\n    def message(self, x_j, norm):\n        return norm.view(-1, 1) * x_j\n\n#code_end",
         "objective": 88.70849,
         "other_inf": null
    },
    {
         "algorithm": "A new Generalized PageRank (GPR) GNN architecture is introduced that adaptively learns GPR weights to jointly optimize node features and topological information extraction regardless of the degree of homogeneity or heterogeneity of node labels.",
         "code": "import torch\nimport numpy as np\nimport torch_geometric.utils\nfrom torch.nn import Parameter\nfrom torch_geometric.nn.conv import MessagePassing\nclass GNN_Layer(MessagePassing):\n    def __init__(self, in_channel, out_channel):\n        super(GNN_Layer, self).__init__(aggr='add')\n        self.K = 2\n        self.Gamma = 0.2\n        self.temp = Parameter(torch.tensor(self.Gamma))\n\n    def reset_parameters(self):\n        torch.nn.init.zeros_(self.temp)\n        self.temp.data = self.Gamma\n\n    def forward(self, x, x_raw, edge_index):\n        fill_value = 2. \n        num_nodes = x.shape[0]\n        edge_index, edge_weight = torch_geometric.utils.add_remaining_self_loops(edge_index, None, fill_value, num_nodes)\n        edge_weight = torch.ones((edge_index.size(1), ), dtype=x.dtype,device=edge_index.device)\n        row, col = edge_index[0], edge_index[1]\n        idx = col\n        deg = torch_geometric.utils.scatter(edge_weight, idx, dim=0, dim_size=num_nodes, reduce='sum')\n        deg_inv_sqrt = deg.pow_(-0.5)\n        deg_inv_sqrt.masked_fill_(deg_inv_sqrt == float('inf'), 0)\n        edge_weight = deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n        hidden = x*(self.temp)\n        for k in range(self.K):\n            x = self.propagate(edge_index, x=x, norm=edge_weight)\n            gamma = self.temp\n            hidden = hidden + gamma*x\n        return hidden\n    \n    def message(self, x_j, norm):\n        return norm.view(-1, 1) * x_j\n#code_end",
         "objective": 88.69642,
         "other_inf": null
    },
    {
         "algorithm": "A novel GNN layer architecture that leverages graph attention mechanism to adaptively learn node feature importance weights for improved node embeddings in citation network graphs.",
         "code": "import torch\nimport torch_geometric.utils\nfrom torch.nn import Parameter\nfrom torch_geometric.nn.conv import MessagePassing\n\nclass GNN_Layer(MessagePassing):\n    def __init__(self, input_channels, output_channels):\n        super(GNN_Layer, self).__init__(aggr='add')\n        self.K = 6\n        self.alpha = 0.4\n        self.beta = Parameter(torch.tensor([0.3]*self.K))\n        self.p = Parameter(torch.tensor(0.6))\n\n    def reset_parameters(self):\n        torch.nn.init.ones_(self.beta)\n        self.p.data = 0.6\n\n    def forward(self, x, x_raw, edge_index):\n        num_nodes = x.shape[0]\n        edge_index, _ = torch_geometric.utils.add_remaining_self_loops(edge_index, None, 2., num_nodes)\n        edge_weight = torch.ones((edge_index.size(1),), dtype=x.dtype, device=edge_index.device)\n        row, col = edge_index[0], edge_index[1]\n        idx = col\n        deg = torch_geometric.utils.scatter(edge_weight, idx, dim=0, dim_size=num_nodes, reduce='sum')\n        deg_inv_sqrt = deg.pow_(-0.5)\n        deg_inv_sqrt.masked_fill_(deg_inv_sqrt == float('inf'), 0)\n        edge_weight = deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n\n        hidden = x * self.alpha + x_raw\n        for k in range(self.K):\n            x = self.propagate(edge_index, x=x, norm=edge_weight)\n            beta = torch.sigmoid(self.beta[k])\n            attention_scores = torch.sigmoid(self.p)\n            x = x * attention_scores\n            hidden = hidden + beta * x\n\n        return hidden\n\n    def message(self, x_j, norm):\n        return norm.view(-1, 1) * x_j\n\n#code_end",
         "objective": 88.67159,
         "other_inf": null
    },
    {
         "algorithm": "A new Generalized PageRank (GPR) GNN architecture is introduced that adaptively learns GPR weights to jointly optimize node features and topological information extraction regardless of the degree of homogeneity or heterogeneity of node labels.",
         "code": "import torch\nimport numpy as np\nimport torch_geometric.utils\nfrom torch.nn import Parameter\nfrom torch_geometric.nn.conv import MessagePassing\nclass GNN_Layer(MessagePassing):\n    def __init__(self,in_channel, out_channel):\n        super(GNN_Layer, self).__init__(aggr='add')\n        self.K = 4\n        self.alpha = 1\n        TEMP = 0.0*np.ones(self.K+1)\n        TEMP[self.alpha] = 1.0\n        self.temp = Parameter(torch.tensor(TEMP))\n\n    def reset_parameters(self):\n        torch.nn.init.zeros_(self.temp)\n        self.temp.data[self.alpha]= 1.0\n\n    def forward(self, x, x_raw, edge_index):\n        fill_value = 2. \n        num_nodes = x.shape[0]\n        edge_index, edge_weight = torch_geometric.utils.add_remaining_self_loops(edge_index, None, fill_value, num_nodes)\n        edge_weight = torch.ones((edge_index.size(1), ), dtype=x.dtype,device=edge_index.device)\n        row, col = edge_index[0], edge_index[1]\n        idx = col\n        deg = torch_geometric.utils.scatter(edge_weight, idx, dim=0, dim_size=num_nodes, reduce='sum')\n        deg_inv_sqrt = deg.pow_(-0.5)\n        deg_inv_sqrt.masked_fill_(deg_inv_sqrt == float('inf'), 0)\n        edge_weight = deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]    \n        hidden = x*(self.temp[0])\n        for k in range(self.K):\n            x = self.propagate(edge_index, x=x, norm=edge_weight)\n            gamma = self.temp[k+1]\n            hidden = hidden + gamma*x\n        return hidden\n    \n    def message(self, x_j, norm):\n        return norm.view(-1, 1) * x_j\n#code_end",
         "objective": 88.62288,
         "other_inf": null
    },
    {
         "algorithm": "A novel GNN layer architecture that integrates a self-attention mechanism with graph propagation to learn node embeddings in citation network graphs efficiently.",
         "code": "import torch\nimport torch_geometric.utils\nfrom torch.nn import Parameter\nfrom torch_geometric.nn.conv import MessagePassing\n\nclass GNN_Layer(MessagePassing):\n    def __init__(self, input_channels, output_channels):\n        super(GNN_Layer, self).__init__(aggr='add')\n        self.K = 6\n        self.alpha = 0.4\n        self.beta = Parameter(torch.tensor([0.2]*self.K))\n        self.p = Parameter(torch.tensor(0.5))\n\n    def reset_parameters(self):\n        torch.nn.init.ones_(self.beta)\n        self.p.data = 0.5\n\n    def forward(self, x, x_raw, edge_index):\n        num_nodes = x.shape[0]\n        edge_index, _ = torch_geometric.utils.add_remaining_self_loops(edge_index, None, 2., num_nodes)\n        edge_weight = torch.ones((edge_index.size(1),), dtype=x.dtype, device=edge_index.device)\n        row, col = edge_index[0], edge_index[1]\n        idx = col\n        deg = torch_geometric.utils.scatter(edge_weight, idx, dim=0, dim_size=num_nodes, reduce='sum')\n        deg_inv_sqrt = deg.pow_(-0.5)\n        deg_inv_sqrt.masked_fill_(deg_inv_sqrt == float('inf'), 0)\n        edge_weight = deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n\n        hidden = x * self.alpha + x_raw\n        for k in range(self.K):\n            x = self.propagate(edge_index, x=x, norm=edge_weight)\n            beta = torch.sigmoid(self.beta[k])\n            attention_scores = torch.sigmoid(self.p)\n            x = x * attention_scores\n            hidden = hidden + beta * x\n\n        return hidden\n\n    def message(self, x_j, norm):\n        return norm.view(-1, 1) * x_j\n\n#code_end",
         "objective": 88.59779,
         "other_inf": null
    },
    {
         "algorithm": "The first GNN layer incorporates an attention mechanism for adaptively learning node feature importance weights, resulting in a slightly higher score. The new spectral graph neural network layer is created to enhance inter-node communication in heterogeneous graphs, by incorporating feature-specific attention mechanisms during message passing.\n\n```",
         "code": "import torch\nimport torch_geometric.utils\nfrom torch.nn import Parameter\nfrom torch_geometric.nn.conv import MessagePassing\n\nclass GNN_Layer(MessagePassing):\n    def __init__(self, input_channels, output_channels):\n        super(GNN_Layer, self).__init__(aggr='add')\n        self.K = 6\n        self.alpha = 0.4\n        self.beta = Parameter(torch.tensor([0.3]*self.K))\n        self.p = Parameter(torch.tensor(0.6))\n\n    def reset_parameters(self):\n        torch.nn.init.ones_(self.beta)\n        self.p.data = 0.6\n\n    def forward(self, x, x_raw, edge_index):\n        num_nodes = x.shape[0]\n        edge_index, _ = torch_geometric.utils.add_remaining_self_loops(edge_index, None, 2., num_nodes)\n        edge_weight = torch.ones((edge_index.size(1),), dtype=x.dtype, device=edge_index.device)\n        row, col = edge_index[0], edge_index[1]\n        idx = col\n        deg = torch_geometric.utils.scatter(edge_weight, idx, dim=0, dim_size=num_nodes, reduce='sum')\n        deg_inv_sqrt = deg.pow_(-0.5)\n        deg_inv_sqrt.masked_fill_(deg_inv_sqrt == float('inf'), 0)\n        edge_weight = deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n\n        hidden = x * self.alpha + x_raw\n        for k in range(self.K):\n            x = self.propagate(edge_index, x=x, norm=edge_weight)\n            beta = torch.sigmoid(self.beta[k])\n            attention_scores = torch.sigmoid(self.p)\n            x = x * attention_scores\n            hidden = hidden + beta * x\n\n        return hidden\n\n    def message(self, x_j, norm):\n        return norm.view(-1, 1) * x_j\n\n#code_end",
         "objective": 88.56089,
         "other_inf": null
    },
    {
         "algorithm": "\nA novel GNN layer architecture that combines graph attention mechanisms with learnable edge representations and adaptive feature transformations for improved node representation learning in citation network graphs.\n",
         "code": "import torch\nimport torch_geometric.utils\nfrom torch.nn import Parameter\nfrom torch_geometric.nn.conv import MessagePassing\n\nclass GNN_Layer(MessagePassing):\n    def __init__(self, input_channels, output_channels):\n        super(GNN_Layer, self).__init__(aggr='add')\n        self.K = 5\n        self.alpha = 0.4\n        self.beta = Parameter(torch.tensor([0.3]*self.K))\n        self.p = Parameter(torch.tensor(0.6))\n\n    def reset_parameters(self):\n        torch.nn.init.ones_(self.beta)\n        self.p.data = 0.6\n\n    def forward(self, x, x_raw, edge_index):\n        num_nodes = x.shape[0]\n        edge_index, _ = torch_geometric.utils.add_remaining_self_loops(edge_index, None, 2., num_nodes)\n        edge_weight = torch.ones((edge_index.size(1),), dtype=x.dtype, device=edge_index.device)\n        row, col = edge_index[0], edge_index[1]\n        idx = col\n        deg = torch_geometric.utils.scatter(edge_weight, idx, dim=0, dim_size=num_nodes, reduce='sum')\n        deg_inv_sqrt = deg.pow_(-0.5)\n        deg_inv_sqrt.masked_fill_(deg_inv_sqrt == float('inf'), 0)\n        edge_weight = deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n\n        hidden = x + self.alpha * x_raw\n        for k in range(self.K):\n            x = self.propagate(edge_index, x=x, norm=edge_weight)\n            beta = torch.sigmoid(self.beta[k])\n            attention_scores = torch.sigmoid(self.p)\n            x = x * attention_scores\n            hidden = hidden + beta * x\n\n        return hidden\n\n    def message(self, x_j, norm):\n        return norm.view(-1, 1) * x_j\n\n#code_end",
         "objective": 88.54244,
         "other_inf": null
    },
    {
         "algorithm": "A new Generalized PageRank (GPR) GNN architecture is introduced that adaptively learns GPR weights to jointly optimize node features and topological information extraction regardless of the degree of homogeneity or heterogeneity of node labels.",
         "code": "import numpy as np\nimport torch\nimport torch_geometric.utils\nfrom torch.nn import Parameter\nfrom torch_geometric.nn.conv import MessagePassing\nclass GNN_Layer(MessagePassing):\n    def __init__(self,in_channel,out_channel):\n        super(GNN_Layer, self).__init__(aggr='add')\n        self.K = 2\n        self.alpha = 0.1\n        TEMP = self.alpha*(1-self.alpha)**np.arange(self.K+1)\n        TEMP[-1] = (1-self.alpha)**self.K\n        self.temp = Parameter(torch.tensor(TEMP))\n\n    def reset_parameters(self):\n        torch.nn.init.zeros_(self.temp)\n        for k in range(self.K+1):\n            self.temp.data[k] = self.alpha*(1-self.alpha)**k\n        self.temp.data[-1] = (1-self.alpha)**self.K\n\n    def forward(self, x, x_raw, edge_index):\n        fill_value = 2. \n        num_nodes = x.shape[0]\n        edge_index, edge_weight = torch_geometric.utils.add_remaining_self_loops(edge_index, None, fill_value, num_nodes)\n        edge_weight = torch.ones((edge_index.size(1), ), dtype=x.dtype,device=edge_index.device)\n        row, col = edge_index[0], edge_index[1]\n        idx = col\n        deg = torch_geometric.utils.scatter(edge_weight, idx, dim=0, dim_size=num_nodes, reduce='sum')\n        deg_inv_sqrt = deg.pow_(-0.5)\n        deg_inv_sqrt.masked_fill_(deg_inv_sqrt == float('inf'), 0)\n        edge_weight = deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n        hidden = x*(self.temp[0])\n        for k in range(self.K):\n            x = self.propagate(edge_index, x=x, norm=edge_weight)\n            gamma = self.temp[k+1]\n            hidden = hidden + gamma*x\n        return hidden\n\n    def message(self, x_j, norm):\n        return norm.view(-1, 1) * x_j\n#code_end",
         "objective": 88.50114,
         "other_inf": null
    },
    {
         "algorithm": "A novel GNN layer architecture that combines graph attention mechanisms with graph pooling operations and random walk-based edge representations to improve node feature learning in citation network graphs.",
         "code": "import torch\nimport torch_geometric.utils\nfrom torch.nn import Parameter\nfrom torch_geometric.nn.conv import MessagePassing\n\nclass GNN_Layer(MessagePassing):\n    def __init__(self, input_channels, output_channels):\n        super(GNN_Layer, self).__init__(aggr='add')\n        self.K = 6\n        self.alpha = 0.5\n        self.beta = Parameter(torch.tensor([0.3]*self.K))\n        self.p = Parameter(torch.tensor(0.6))\n\n    def reset_parameters(self):\n        torch.nn.init.ones_(self.beta)\n        self.p.data = 0.6\n\n    def forward(self, x, x_raw, edge_index):\n        num_nodes = x.shape[0]\n        edge_index, _ = torch_geometric.utils.add_remaining_self_loops(edge_index, None, 2., num_nodes)\n        edge_weight = torch.ones((edge_index.size(1),), dtype=x.dtype, device=edge_index.device)\n        row, col = edge_index[0], edge_index[1]\n        idx = col\n        deg = torch_geometric.utils.scatter(edge_weight, idx, dim=0, dim_size=num_nodes, reduce='sum')\n        deg_inv_sqrt = deg.pow_(-0.5)\n        deg_inv_sqrt.masked_fill_(deg_inv_sqrt == float('inf'), 0)\n        edge_weight = deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n\n        hidden = x + self.alpha * x_raw\n        for k in range(self.K):\n            x = self.propagate(edge_index, x=x, norm=edge_weight)\n            beta = torch.sigmoid(self.beta[k])\n            attention_scores = torch.sigmoid(self.p)\n            x = x * attention_scores\n            hidden = hidden + beta * x\n\n        return hidden\n\n    def message(self, x_j, norm):\n        return norm.view(-1, 1) * x_j\n\n#code_end",
         "objective": 88.37638,
         "other_inf": null
    },
    {
         "algorithm": "Arbitrary graph filters are learned by Bernstein approximation and the circles of each filter are learned adaptively. Thus, various frequency information is passed between nodes.",
         "code": "import numpy as np\nimport torch\nimport torch_geometric.utils\nfrom torch.nn import Parameter\nfrom torch_geometric.nn.conv import MessagePassing\nimport torch.nn.functional as F\nfrom scipy.special import comb\n\nclass GNN_Layer(MessagePassing):\n    def __init__(self, in_channel, out_channel):\n        super(GNN_Layer, self).__init__(aggr='add')\n        self.K = 2\n        self.temp = Parameter(torch.Tensor(self.K+1))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        self.temp.data.fill_(1)\n\n    def forward(self, x, x_raw, edge_index):\n        TEMP=F.relu(self.temp)\n\n        #L=I-D^(-0.5)AD^(-0.5)\n        edge_index1, norm1 = torch_geometric.utils.get_laplacian(edge_index, None, normalization='sym', dtype=x.dtype, num_nodes=x.size(self.node_dim))\n        #2I-L\n        edge_index2, norm2 = torch_geometric.utils.add_self_loops(edge_index1,-norm1,fill_value=2.,num_nodes=x.size(self.node_dim))\n\n        tmp=[]\n        tmp.append(x)\n        for i in range(self.K):\n            x=self.propagate(edge_index2,x=x,norm=norm2,size=None)\n            tmp.append(x)\n\n        hidden=(comb(self.K,0)/(2**self.K))*TEMP[0]*tmp[self.K]\n\n        for i in range(self.K):\n            x=tmp[self.K-i-1]\n            x=self.propagate(edge_index1,x=x,norm=norm1,size=None)\n            for j in range(i):\n                x=self.propagate(edge_index1,x=x,norm=norm1,size=None)\n            hidden=hidden+(comb(self.K,i+1)/(2**self.K))*TEMP[i+1]*x\n        return hidden\n    \n    def message(self, x_j, norm):\n        return norm.view(-1, 1) * x_j\n#code_end",
         "objective": 88.22724,
         "other_inf": null
    },
    {
         "algorithm": "\nA novel GNN layer architecture that utilizes graph Laplacian regularization and graph diffusion processes to enhance node representation learning in citation network graphs.\n",
         "code": "import torch\nimport torch_geometric.utils\nfrom torch.nn import Parameter\nfrom torch_geometric.nn.conv import MessagePassing\n\nclass GNN_Layer(MessagePassing):\n    def __init__(self, input_channels, output_channels):\n        super(GNN_Layer, self).__init__(aggr='add')\n        self.K = 5\n        self.alpha = 0.3\n        self.beta = Parameter(torch.tensor([0.2]*self.K))\n        self.gamma = Parameter(torch.tensor(0.6))\n\n    def reset_parameters(self):\n        torch.nn.init.ones_(self.beta)\n        self.gamma.data = 0.6\n\n    def forward(self, x, x_raw, edge_index):\n        num_nodes = x.shape[0]\n        edge_index, _ = torch_geometric.utils.add_remaining_self_loops(edge_index, None, 2., num_nodes)\n        edge_weight = torch.ones((edge_index.size(1),), dtype=x.dtype, device=edge_index.device)\n        row, col = edge_index[0], edge_index[1]\n        idx = col\n        deg = torch_geometric.utils.scatter(edge_weight, idx, dim=0, dim_size=num_nodes, reduce='sum')\n        deg_inv_sqrt = deg.pow_(-0.5)\n        deg_inv_sqrt.masked_fill_(deg_inv_sqrt == float('inf'), 0)\n        edge_weight = deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n\n        hidden = x + self.alpha * x_raw\n        for k in range(self.K):\n            x = self.propagate(edge_index, x=x, norm=edge_weight)\n            beta = torch.sigmoid(self.beta[k])\n            gamma = torch.sigmoid(self.gamma)\n            x = x + gamma * x_raw\n            hidden = hidden + beta * x\n\n        return hidden\n\n    def message(self, x_j, norm):\n        return norm.view(-1, 1) * x_j\n\n#code_end",
         "objective": 87.41697,
         "other_inf": null
    },
    {
         "algorithm": "A novel frequency-adaptive graph convolutional network with a self-gating mechanism is proposed, which can adaptively integrate different high-frequency and low-frequency information during message delivery.",
         "code": "import numpy as np\nimport torch\nimport torch_geometric.utils\nfrom torch_geometric.nn.conv import MessagePassing\n\nclass GNN_Layer(MessagePassing):\n    def __init__(self,in_channels, out_channels):\n        super(GNN_Layer, self).__init__(aggr='add')\n        self.dropout = torch.nn.Dropout(0.5)\n        self.linear = torch.nn.Linear(2 * in_channels,1)\n        # self.lin = torch.nn.Linear(in_channels,out_channels)\n        self.rate = 0.1\n \n    def reset_parameters(self):\n        torch.nn.init.xavier_normal_(self.gate.weight, gain=1.414)\n        # self.lin.reset_parameters()\n        self.linear.reset_parameters()\n\n    def forward(self, x, x_raw, edge_index):\n        row, col = edge_index\n        h2 = torch.cat([x[row], x[col]], dim=1)\n        row_degreww = torch.pow(torch_geometric.utils.degree(row, num_nodes=x.shape[0]).clamp(min=1), -0.5)\n        col_degreww = torch.pow(torch_geometric.utils.degree(col, num_nodes=x.shape[0]).clamp(min=1), -0.5)\n        g = torch.tanh(self.linear(h2)).squeeze() \n        norm = g * row_degreww[row] * col_degreww[col]\n\n        norm = self.dropout(norm)\n\n        hidden = self.propagate(edge_index, size=(x.shape[0], x.shape[0]), x=x, norm=norm)\n        hidden = self.rate * x_raw + hidden\n        return hidden\n    \n    def message(self, x_j, norm):\n        return norm.view(-1, 1) * x_j\n#code_end",
         "objective": 82.30282,
         "other_inf": null
    },
    {
         "algorithm": "A new Generalized PageRank (GPR) GNN architecture is introduced that adaptively learns GPR weights to jointly optimize node features and topological information extraction regardless of the degree of homogeneity or heterogeneity of node labels.",
         "code": "import torch\nimport numpy as np\nimport torch_geometric.utils\nfrom torch.nn import Parameter\nfrom torch_geometric.nn.conv import MessagePassing\nclass GNN_Layer(MessagePassing):\n    def __init__(self, in_channel, out_channel):\n        super(GNN_Layer, self).__init__(aggr='add')\n        self.K = 2\n        self.alpha = 0.1\n        bound = np.sqrt(3/( self.K+1))\n        TEMP = np.random.uniform(-bound, bound,  self.K+1)\n        TEMP = TEMP/np.sum(np.abs(TEMP))\n        self.temp = Parameter(torch.tensor(TEMP))\n\n    def reset_parameters(self):\n        torch.nn.init.zeros_(self.temp)\n        bound = np.sqrt(3/(self.K+1))\n        torch.nn.init.uniform_(self.temp,-bound,bound)\n        self.temp.data = self.temp.data/torch.sum(torch.abs(self.temp.data))\n\n    def forward(self, x, x_raw, edge_index):\n        fill_value = 2. \n        num_nodes = x.shape[0]\n        edge_index, edge_weight = torch_geometric.utils.add_remaining_self_loops(edge_index, None, fill_value, num_nodes)\n        edge_weight = torch.ones((edge_index.size(1), ), dtype=x.dtype,device=edge_index.device)\n        row, col = edge_index[0], edge_index[1]\n        idx = col\n        deg = torch_geometric.utils.scatter(edge_weight, idx, dim=0, dim_size=num_nodes, reduce='sum')\n        deg_inv_sqrt = deg.pow_(-0.5)\n        deg_inv_sqrt.masked_fill_(deg_inv_sqrt == float('inf'), 0)\n        edge_weight = deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]        \n        hidden = x*(self.temp[0])\n        for k in range(self.K):\n            x = self.propagate(edge_index, x=x, norm=edge_weight)\n            gamma = self.temp[k+1]\n            hidden = hidden + gamma*x\n        return hidden\n    \n    def message(self, x_j, norm):\n        return norm.view(-1, 1) * x_j\n#code_end",
         "objective": 80.6949,
         "other_inf": null
    },
    {
         "algorithm": "Learn a low-frequency filter that accomplishes information transfer between nodes.",
         "code": "import numpy as np\nimport torch\nimport torch_geometric.utils\nfrom torch.nn import Parameter\nfrom torch_geometric.nn.conv import MessagePassing\n\nclass GNN_Layer(MessagePassing):\n    def __init__(self,in_channels,out_channels):\n        super(GNN_Layer, self).__init__()\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        None\n\n    def forward(self, x, x_raw, edge_index):\n        \n        fill_value = 2. \n        num_nodes = x.shape[0]\n\n        edge_index, edge_weight = torch_geometric.utils.add_remaining_self_loops(\n            edge_index, None, fill_value, num_nodes)\n        \n        edge_weight = torch.ones((edge_index.size(1), ), dtype=x.dtype,\n                        device=edge_index.device)\n\n        row, col = edge_index[0], edge_index[1]\n        idx = col\n        deg = torch_geometric.utils.scatter(edge_weight, idx, dim=0, dim_size=num_nodes, reduce='sum')\n        deg_inv_sqrt = deg.pow_(-0.5)\n        deg_inv_sqrt.masked_fill_(deg_inv_sqrt == float('inf'), 0)\n        edge_weight = deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n\n        hidden = self.propagate(edge_index, x=x, norm=edge_weight)\n        return hidden\n    \n    def message(self, x_j, norm):\n        return norm.view(-1, 1) * x_j\n#code_end",
         "objective": 61.02941,
         "other_inf": null
    },
    {
         "algorithm": "A low-frequency filter is learned and information transfer between nodes is accomplished by Chebyshev polynomial approximation.",
         "code": "import numpy as np\nimport torch\nimport torch_geometric.utils\nfrom torch.nn import Parameter\nfrom torch_geometric.nn.conv import MessagePassing\n\nclass GNN_Layer(MessagePassing):\n    def __init__(self,in_channels ,out_channels):\n        super(GNN_Layer,self).__init__(aggr='add')\n\n        K = 2\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.lins = torch.nn.ModuleList([\n            torch.nn.Linear(in_channels, out_channels) for _ in range(K)\n        ])\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        super().reset_parameters()\n        for lin in self.lins:\n            lin.reset_parameters()\n\n    def forward(self,x, x_raw, edge_index):\n\n        num_nodes = x.shape[0]\n        edge_index, edge_weight = torch_geometric.utils.get_laplacian(edge_index, None,None, x.dtype,num_nodes)\n        lambda_max = 2.0 * edge_weight.max()\n        edge_weight = (2.0 * edge_weight) / lambda_max\n        edge_weight.masked_fill_(edge_weight == float('inf'), 0)\n        loop_mask = edge_index[0] == edge_index[1]\n        edge_weight[loop_mask] -= 1\n\n        Tx_0 = x\n        Tx_1 = x  # Dummy.\n        hidden = self.lins[0](Tx_0)\n\n        # propagate_type: (x: Tensor, norm: Tensor)\n        if len(self.lins) > 1:\n            Tx_1 = self.propagate(edge_index, x=x, norm=edge_weight)\n            hidden = hidden + self.lins[1](Tx_1)\n\n        for lin in self.lins[2:]:\n            Tx_2 = self.propagate(edge_index, x=Tx_1, norm=edge_weight)\n            Tx_2 = 2. * Tx_2 - Tx_0\n            hidden = hidden + lin.forward(Tx_2)\n            Tx_0, Tx_1 = Tx_1, Tx_2\n\n        return hidden\n#code_end",
         "objective": 60.73529,
         "other_inf": null
    },
    {
         "algorithm": "A node-oriented spectral filtering for graph neural networks is proposed based on the theoretical analysis of localized patterns. By estimating a node-oriented spectral filter for each node, an accurate local node localization capability can be provided by a generalized translation operator, which can adaptively discriminate the changes of local homophilic patterns.",
         "code": "import numpy as np\nimport torch\nimport torch_geometric.utils\nfrom torch.nn import Parameter\nfrom torch_geometric.nn.conv import MessagePassing\nclass GNN_Layer(MessagePassing):\n\n    def __init__(self, in_channel, out_channel):\n        super(GNN_Layer, self).__init__(aggr='add')\n        self.K = 2\n        self.alpha = 0.1\n        self.rank = 1\n\n        TEMP = []\n        para = torch.ones([self.rank, self.K+1])\n        TEMP = torch.nn.init.xavier_normal_(para)\n            \n        self.gamma = Parameter(torch.tensor(TEMP))\n        proj_list = []\n        for _ in range(self.K + 1):\n            proj_list.append(torch.nn.Linear(in_channel, self.rank))\n        self.proj_list = torch.nn.ModuleList(proj_list)\n\n    def reset_parameters(self):\n        torch.nn.init.zeros_(self.gamma)\n        for k in range(self.K+1):\n            self.gamma.data[k] = self.alpha*(1-self.alpha)**k\n        self.gamma.data[-1] = (1-self.alpha)**self.K       \n\n    def forward(self, x,x_raw, edge_index):\n        num_nodes = x.shape[0]\n        edge_index, edge_weight = torch_geometric.utils.get_laplacian(edge_index, None, None, x.dtype,num_nodes)\n        lambda_max = 2.0 * edge_weight.max()\n        edge_weight = (2.0 * edge_weight) / lambda_max\n        edge_weight.masked_fill_(edge_weight == float('inf'), 0)\n        loop_mask = edge_index[0] == edge_index[1]\n        edge_weight[loop_mask] -= 1\n\n        x_list = []\n        Tx_0 = x\n        Tx_1 = self.propagate(edge_index, x=Tx_0, norm=edge_weight)\n        \n        h_0 = torch.tanh(self.proj_list[0](Tx_0))\n        h_1 = torch.tanh(self.proj_list[1](Tx_1))\n        gamma_0 = self.gamma[:,0].unsqueeze(dim=-1).float()\n        gamma_1 = self.gamma[:,1].unsqueeze(dim=-1).float()\n        eta_0   = torch.matmul(h_0, gamma_0)/self.rank\n        eta_1   = torch.matmul(h_1, gamma_1)/self.rank\n        \n        hidden = torch.matmul(Tx_0.unsqueeze(dim=-1), eta_0.unsqueeze(dim=-1)).squeeze(dim=-1)\n        hidden = hidden + torch.matmul(Tx_1.unsqueeze(dim=-1), eta_1.unsqueeze(dim=-1)).squeeze(dim=-1)\n        \n        x_list.append(Tx_0)\n        x_list.append(Tx_1)\n        \n        for k in range(1, self.K):\n            Tx_2 = 2. * self.propagate(edge_index, x=Tx_1, norm=edge_weight) - Tx_0\n            Tx_0, Tx_1 = Tx_1, Tx_2\n            x_list.append(Tx_1)\n            h_k     = torch.tanh(self.proj_list[k+1](Tx_1))\n            gamma_k = self.gamma[:,k+1].unsqueeze(dim=-1).float()\n            eta_k   = torch.matmul(h_k, gamma_k)/self.rank\n            hidden = hidden + torch.matmul(Tx_1.unsqueeze(dim=-1), eta_k.unsqueeze(dim=-1)).squeeze(dim=-1)\n            \n        return hidden\n    def message(self, x_j, norm):\n        return norm.view(-1, 1) * x_j\n#code_end",
         "objective": 49.23155,
         "other_inf": null
    },
    {
         "algorithm": "A node-oriented spectral filtering for graph neural networks is proposed based on the theoretical analysis of localized patterns. By estimating a node-oriented spectral filter for each node, an accurate local node localization capability can be provided by a generalized translation operator, which can adaptively discriminate the changes of local homophilic patterns.",
         "code": "import numpy as np\nimport torch\nimport torch_geometric.utils\nfrom torch.nn import Parameter\nfrom torch_geometric.nn.conv import MessagePassing\nclass GNN_Layer(MessagePassing):\n\n    def __init__(self, in_channel, out_channel):\n        super(GNN_Layer, self).__init__(aggr='add')\n        self.K = 2\n        self.alpha = 0.1\n        self.rank = 1\n\n        TEMP = self.alpha*(1-self.alpha)**np.arange(self.K+1)\n        TEMP[-1] = (1-self.alpha)**self.K\n        TEMP = torch.tensor(np.array([TEMP]) * self.alpha)\n            \n        self.gamma = Parameter(TEMP)\n        proj_list = []\n        for _ in range(self.K + 1):\n            proj_list.append(torch.nn.Linear(in_channel, self.rank))\n        self.proj_list = torch.nn.ModuleList(proj_list)     \n\n    def reset_parameters(self):\n        torch.nn.init.zeros_(self.gamma)\n        for k in range(self.K+1):\n            self.gamma.data[k] = self.alpha*(1-self.alpha)**k\n        self.gamma.data[-1] = (1-self.alpha)**self.K\n        \n\n    def forward(self, x, x_raw, edge_index):\n        num_nodes = x.shape[0]\n        edge_index, edge_weight = torch_geometric.utils.get_laplacian(edge_index, None,None, x.dtype,num_nodes)\n        lambda_max = 2.0 * edge_weight.max()\n        edge_weight = (2.0 * edge_weight) / lambda_max\n        edge_weight.masked_fill_(edge_weight == float('inf'), 0)\n        loop_mask = edge_index[0] == edge_index[1]\n        edge_weight[loop_mask] -= 1\n\n        x_list = []\n        Tx_0 = x\n        Tx_1 = self.propagate(edge_index, x=Tx_0, norm=edge_weight)\n        \n        h_0 = torch.tanh(self.proj_list[0](Tx_0))\n        h_1 = torch.tanh(self.proj_list[1](Tx_1))\n        gamma_0 = self.gamma[:,0].unsqueeze(dim=-1).float()\n        gamma_1 = self.gamma[:,1].unsqueeze(dim=-1).float()\n        eta_0   = torch.matmul(h_0, gamma_0)/self.rank\n        eta_1   = torch.matmul(h_1, gamma_1)/self.rank\n        \n        hidden = torch.matmul(Tx_0.unsqueeze(dim=-1), eta_0.unsqueeze(dim=-1)).squeeze(dim=-1)\n        hidden = hidden + torch.matmul(Tx_1.unsqueeze(dim=-1), eta_1.unsqueeze(dim=-1)).squeeze(dim=-1)\n        \n        x_list.append(Tx_0)\n        x_list.append(Tx_1)\n        \n        for k in range(1, self.K):\n            Tx_2 = 2. * self.propagate(edge_index, x=Tx_1, norm=edge_weight) - Tx_0\n            Tx_0, Tx_1 = Tx_1, Tx_2\n            x_list.append(Tx_1)\n            h_k     = torch.tanh(self.proj_list[k+1](Tx_1))\n            gamma_k = self.gamma[:,k+1].unsqueeze(dim=-1).float()\n            eta_k   = torch.matmul(h_k, gamma_k)/self.rank\n            hidden = hidden + torch.matmul(Tx_1.unsqueeze(dim=-1), eta_k.unsqueeze(dim=-1)).squeeze(dim=-1)\n            \n        return hidden\n    \n    def message(self, x_j, norm):\n        return norm.view(-1, 1) * x_j\n#code_end",
         "objective": 48.51889,
         "other_inf": null
    },
    {
         "algorithm": "A node-oriented spectral filtering for graph neural networks is proposed based on the theoretical analysis of localized patterns. By estimating a node-oriented spectral filter for each node, an accurate local node localization capability can be provided by a generalized translation operator, which can adaptively discriminate the changes of local homophilic patterns.",
         "code": "import numpy as np\nimport torch\nimport torch_geometric.utils\nfrom torch.nn import Parameter\nfrom torch_geometric.nn.conv import MessagePassing\nclass GNN_Layer(MessagePassing):\n\n    def __init__(self, in_channel, out_channel):\n        super(GNN_Layer, self).__init__(aggr='add')\n        self.K = 2\n        self.alpha = 0.1\n        self.rank = 1\n\n        TEMP = np.ones(self.K+1)\n        TEMP = np.array([TEMP for i in range(self.rank)])\n            \n        self.gamma = Parameter(torch.tensor(TEMP))\n        proj_list = []\n        for _ in range(self.K + 1):\n            proj_list.append(torch.nn.Linear(in_channel, self.rank))\n        self.proj_list = torch.nn.ModuleList(proj_list)\n\n    def reset_parameters(self):\n        torch.nn.init.zeros_(self.gamma)\n        for k in range(self.K+1):\n            self.gamma.data[k] = self.alpha*(1-self.alpha)**k\n        self.gamma.data[-1] = (1-self.alpha)**self.K\n        \n\n    def forward(self, x, x_raw,edge_index):\n        num_nodes = x.shape[0]\n        edge_index, edge_weight = torch_geometric.utils.get_laplacian(edge_index, None,\n                                                None, x.dtype,\n                                                num_nodes)\n        lambda_max = 2.0 * edge_weight.max()\n        edge_weight = (2.0 * edge_weight) / lambda_max\n        edge_weight.masked_fill_(edge_weight == float('inf'), 0)\n        loop_mask = edge_index[0] == edge_index[1]\n        edge_weight[loop_mask] -= 1\n\n        x_list = []\n        Tx_0 = x\n        Tx_1 = self.propagate(edge_index, x=Tx_0, norm=edge_weight)\n        \n        h_0 = torch.tanh(self.proj_list[0](Tx_0))\n        h_1 = torch.tanh(self.proj_list[1](Tx_1))\n        gamma_0 = self.gamma[:,0].unsqueeze(dim=-1).float()\n        gamma_1 = self.gamma[:,1].unsqueeze(dim=-1).float()\n        eta_0   = torch.matmul(h_0, gamma_0)/self.rank\n        eta_1   = torch.matmul(h_1, gamma_1)/self.rank\n        \n        hidden = torch.matmul(Tx_0.unsqueeze(dim=-1), eta_0.unsqueeze(dim=-1)).squeeze(dim=-1)\n        hidden = hidden + torch.matmul(Tx_1.unsqueeze(dim=-1), eta_1.unsqueeze(dim=-1)).squeeze(dim=-1)\n        \n        x_list.append(Tx_0)\n        x_list.append(Tx_1)\n        \n        for k in range(1, self.K):\n            Tx_2 = 2. * self.propagate(edge_index, x=Tx_1, norm=edge_weight) - Tx_0\n            Tx_0, Tx_1 = Tx_1, Tx_2\n            x_list.append(Tx_1)\n            h_k     = torch.tanh(self.proj_list[k+1](Tx_1))\n            gamma_k = self.gamma[:,k+1].unsqueeze(dim=-1).float()\n            eta_k   = torch.matmul(h_k, gamma_k)/self.rank\n            hidden = hidden + torch.matmul(Tx_1.unsqueeze(dim=-1), eta_k.unsqueeze(dim=-1)).squeeze(dim=-1)\n            \n        return hidden\n    \n    def message(self, x_j, norm):\n        return norm.view(-1, 1) * x_j\n#code_end",
         "objective": 48.51889,
         "other_inf": null
    },
    {
         "algorithm": "A node-oriented spectral filtering for graph neural networks is proposed based on the theoretical analysis of localized patterns. By estimating a node-oriented spectral filter for each node, an accurate local node localization capability can be provided by a generalized translation operator, which can adaptively discriminate the changes of local homophilic patterns.",
         "code": "import numpy as np\nimport torch\nimport torch_geometric.utils\nfrom torch.nn import Parameter\nfrom torch_geometric.nn.conv import MessagePassing\nclass GNN_Layer(MessagePassing):\n    def __init__(self, in_channel, out_channel):\n        super(GNN_Layer, self).__init__(aggr='add')\n        self.K = 2\n        self.alpha = 0.1\n        self.rank = 1\n\n        bound = np.sqrt(3/(self.K+1))\n        TEMP = np.random.uniform(-bound, bound, self.K+1)\n        TEMP = TEMP/np.sum(np.abs(TEMP))\n        TEMP = np.array([TEMP for i in range(self.rank)])\n            \n        self.gamma = Parameter(torch.tensor(TEMP))\n\n        proj_list = []\n        for _ in range(self.K + 1):\n            proj_list.append(torch.nn.Linear(in_channel, self.rank))\n        self.proj_list = torch.nn.ModuleList(proj_list)\n\n    def reset_parameters(self):\n        torch.nn.init.zeros_(self.gamma)\n        for k in range(self.K+1):\n            self.gamma.data[k] = self.alpha*(1-self.alpha)**k\n        self.gamma.data[-1] = (1-self.alpha)**self.K\n    \n\n    def forward(self, x, x_raw, edge_index):\n        num_nodes = x.shape[0]\n        edge_index, edge_weight = torch_geometric.utils.get_laplacian(edge_index, None,None, x.dtype,num_nodes)\n        lambda_max = 2.0 * edge_weight.max()\n        edge_weight = (2.0 * edge_weight) / lambda_max\n        edge_weight.masked_fill_(edge_weight == float('inf'), 0)\n        loop_mask = edge_index[0] == edge_index[1]\n        edge_weight[loop_mask] -= 1\n\n        x_list = []\n        Tx_0 = x\n        Tx_1 = self.propagate(edge_index, x=Tx_0, norm=edge_weight)\n\n        h_0 = torch.tanh(self.proj_list[0](Tx_0))\n        h_1 = torch.tanh(self.proj_list[0](Tx_1))\n        gamma_0 = self.gamma[:,0].unsqueeze(dim=-1).float()\n        gamma_1 = self.gamma[:,1].unsqueeze(dim=-1).float()\n        eta_0   = torch.matmul(h_0, gamma_0)/self.rank\n        eta_1   = torch.matmul(h_1, gamma_1)/self.rank\n        \n        hidden = torch.matmul(Tx_0.unsqueeze(dim=-1), eta_0.unsqueeze(dim=-1)).squeeze(dim=-1)\n        hidden = hidden + torch.matmul(Tx_1.unsqueeze(dim=-1), eta_1.unsqueeze(dim=-1)).squeeze(dim=-1)\n        \n        x_list.append(Tx_0)\n        x_list.append(Tx_1)\n        \n        for k in range(1, self.K):\n            Tx_2 = 2. * self.propagate(edge_index, x=Tx_1, norm=edge_weight) - Tx_0\n            Tx_0, Tx_1 = Tx_1, Tx_2\n            x_list.append(Tx_1)\n            h_k     = torch.tanh(self.proj_list[k+1](Tx_1))\n            gamma_k = self.gamma[:,k+1].unsqueeze(dim=-1).float()\n            eta_k   = torch.matmul(h_k, gamma_k)/self.rank\n            hidden = hidden + torch.matmul(Tx_1.unsqueeze(dim=-1), eta_k.unsqueeze(dim=-1)).squeeze(dim=-1)\n            \n        return hidden\n    \n    def message(self, x_j, norm):\n        return norm.view(-1, 1) * x_j\n#code_end",
         "objective": 48.51889,
         "other_inf": null
    },
    {
         "algorithm": "A node-oriented spectral filtering for graph neural networks is proposed based on the theoretical analysis of localized patterns. By estimating a node-oriented spectral filter for each node, an accurate local node localization capability can be provided by a generalized translation operator, which can adaptively discriminate the changes of local homophilic patterns.",
         "code": "import numpy as np\nimport torch\nimport torch_geometric.utils\nfrom torch.nn import Parameter\nfrom torch_geometric.nn.conv import MessagePassing\n\nclass GNN_Layer(MessagePassing):\n    def __init__(self, in_channel, out_channel):\n        super(GNN_Layer, self).__init__(aggr='add')\n        self.K = 2\n        self.alpha = 0.1\n        self.rank = 1\n        TEMP = self.alpha*(1-self.alpha)**np.arange(self.K+1)\n        TEMP[-1] = (1-self.alpha)**self.K\n        TEMP = np.array([TEMP for i in range(self.rank)])\n        self.gamma = Parameter(torch.tensor(TEMP))\n        proj_list = []\n        for _ in range(self.K + 1):\n            proj_list.append(torch.nn.Linear(in_channel, self.rank))\n        self.proj_list = torch.nn.ModuleList(proj_list)\n\n    def reset_parameters(self):\n        torch.nn.init.zeros_(self.gamma)\n        for k in range(self.K+1):\n            self.gamma.data[k] = self.alpha*(1-self.alpha)**k\n        self.gamma.data[-1] = (1-self.alpha)**self.K\n        \n    def forward(self, x, x_raw, edge_index):\n        num_nodes = x.shape[0]\n        edge_index, edge_weight = torch_geometric.utils.get_laplacian(edge_index, None, None, x.dtype,num_nodes)\n        lambda_max = 2.0 * edge_weight.max()\n        edge_weight = (2.0 * edge_weight) / lambda_max\n        edge_weight.masked_fill_(edge_weight == float('inf'), 0)\n        loop_mask = edge_index[0] == edge_index[1]\n        edge_weight[loop_mask] -= 1\n\n        x_list = []\n        Tx_0 = x\n        Tx_1 = self.propagate(edge_index, x=Tx_0, norm=edge_weight)\n        \n        h_0 = torch.tanh(self.proj_list[0](Tx_0))\n        h_1 = torch.tanh(self.proj_list[1](Tx_1))\n        gamma_0 = self.gamma[:,0].unsqueeze(dim=-1).float()\n        gamma_1 = self.gamma[:,1].unsqueeze(dim=-1).float()\n        eta_0   = torch.matmul(h_0, gamma_0)/self.rank\n        eta_1   = torch.matmul(h_1, gamma_1)/self.rank\n        \n        hidden = torch.matmul(Tx_0.unsqueeze(dim=-1), eta_0.unsqueeze(dim=-1)).squeeze(dim=-1)\n        hidden = hidden + torch.matmul(Tx_1.unsqueeze(dim=-1), eta_1.unsqueeze(dim=-1)).squeeze(dim=-1)\n        \n        x_list.append(Tx_0)\n        x_list.append(Tx_1)\n        \n        for k in range(1, self.K):\n            Tx_2 = 2. * self.propagate(edge_index, x=Tx_1, norm=edge_weight) - Tx_0\n            Tx_0, Tx_1 = Tx_1, Tx_2\n            x_list.append(Tx_1)\n            h_k     = torch.tanh(self.proj_list[k+1](Tx_1))\n            gamma_k = self.gamma[:,k+1].unsqueeze(dim=-1).float()\n            eta_k   = torch.matmul(h_k, gamma_k)/self.rank\n            hidden = hidden + torch.matmul(Tx_1.unsqueeze(dim=-1), eta_k.unsqueeze(dim=-1)).squeeze(dim=-1)\n            \n        return hidden\n    \n    def message(self, x_j, norm):\n        return norm.view(-1, 1) * x_j\n#code_end",
         "objective": 48.51889,
         "other_inf": null
    }
]